---
title: "Bayesian data analysis & cognitive modeling"
subtitle: "04: BDA basics"
author: "Michael Franke"
output:
  ioslides_presentation:
    css: mistyle.css
    smaller: no
    transition: faster
    widescreen: yes
---

```{r setup, include=FALSE, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE, dev.args = list(bg = 'transparent'), fig.align='center')
require('tidyverse')
require('forcats')
theme_set(theme_bw() + theme(plot.background=element_blank()) )
```

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
</script>

```{r, child = "miincludes.Rmd"}

```


## key notions

- basics
    - joint distributions & marginalization
    - conditional probability & Bayes rule
- 3 pillars of Bayesian data analysis:
    - estimation 
    - comparison
    - prediction
- parameter estimation for coin flips
    - conjugate priors
    - highest density interval


## recap

definition of **conditional probability**:

$$P(X \, | \, Y) = \frac{P(X \cap Y)}{P(Y)}$$

definition of Bayes rule: 

$$P(X \, | \, Y) = \frac{P(Y \, | \, X) \ P(X)}{P(Y)}$$

version for data analysis:

$$\underbrace{P(\theta \, | \, D)}_{posterior} \propto \underbrace{P(\theta)}_{prior} \ \underbrace{P(D \, | \, \theta)}_{likelihood}$$




# Bayes rule in multi-D

## proportions of eye & hair color

<!---
|  | blond | brown | red | black
:---:|:---:|:---:|:---:|:---:|
blue  | 0.03 | 0.09 | 0.04 | 0.04 |
green | 0.09 | 0.02 | 0 | 0.05 |
brown | 0.09 | 0.41 | 0.01 | 0.13 |
--->

joint probability distribution as a two-dimensional matrix:

```{r, echo = FALSE, message = FALSE}
  # require('gtools')
  # x = matrix(rdirichlet(1, rep(1, 12)), nrow = 3)
  prob2ds = matrix(c(0.03, 0.09, 0.04, 0.04, 0.09, 0.02, 0, 0.05, 0.09, 0.41, 0.01, 0.13), nrow = 3)
  rownames(prob2ds) = c("blue", 'green', 'brown')
  colnames(prob2ds) = c("blond", 'brown', 'red', 'black')
```

```{r}
prob2ds
```


marginal distribution over eye color:

```{r}
  rowSums(prob2ds)
```

## proportions of eye & hair color

joint probability distribution as a two-dimensional matrix:

```{r}
prob2ds
```

conditional probability given blue eyes:

```{r}
  prob2ds["blue",] %>% (function(x) x/sum(x))
```

## model & data

- single coin flip with unknown success bias $\theta \in \{0, \frac{1}{3}, \frac{1}{2}, \frac{2}{3}, 1\}$
- flat prior beliefs: $P(\theta) = .2\,, \forall \theta$

<span style = "color:white"> &nbsp; </span>


<div style = "float:left; width:45%;">

model likelihood $P(D \, | \, \theta)$:

```{r echo = FALSE}
  likelihood = round(matrix(c(0, 1, 1/3, 2/3, 1/2, 1/2, 2/3, 1/3, 1, 0), nrow = 2),2)
  rownames(likelihood) = c("succ", 'fail')
  colnames(likelihood) = c("t=0", "t=1/3", "t=1/2", "t=2/3", "t=1")
```


```{r}
  likelihood
```

  
</div>
<div style = "float:right; width:45%;">
  
weighing in $P(\theta)$:

```{r}
  prob2d = likelihood  * 0.2
  prob2d
```

</div>  



<div style = "position:absolute; top: 620px; right:60px;">
  back to start: joint-probability distribution as 2d matrix again
</div>


## model, data & Bayesian inference

Bayes rule: $P(\theta \, | \, D) \propto P(\theta) \times P(D \, | \, \theta)$

```{r}
  prob2d
```

<div style = "float:left; width:55%;">

posterior $P(\theta \, | \, \text{heads})$ after one success:

```{r}
  prob2d["succ",] %>% 
    (function(x) x / sum(x))
```

</div>
<div style = "float:right; width:35%;">

```{r, echo = FALSE, fig.width = 3.75, fig.height = 2.75, dev.args = list(bg = 'transparent'), fig.align='center'}
  plotData = data.frame(prior = rep(0.2,5), posterior = prob2d[1,]/sum(prob2d[1,]), theta = c(0, 1/3, 0.5, 2/3, 1))
  plotData = gather(plotData, key = distribution, value = probability, prior, posterior )
  ggplot(plotData, aes(x = theta, y = probability, color = distribution )) + geom_point() + geom_line() + theme(plot.background=element_blank()) + 
    scale_color_manual(values = c("darkgrey", "firebrick"))
```
  
</div>  


# 3 pillars of BDA

## caveat

<span style = "color:white"> &nbsp; </span>


this section is for overview and outlook only

<span style = "color:white"> &nbsp; </span>

we will deal with this in detail later

## estimation

<span style = "color:white"> &nbsp; </span>

given model and data, which parameter values should we believe in?

$$\underbrace{P(\theta \, | \, D)}_{posterior} \propto \underbrace{P(\theta)}_{prior} \ \underbrace{P(D \, | \, \theta)}_{likelihood}$$

## model comparison

which of two models is more likely, given the data?

$$\underbrace{\frac{P(M_1 \mid D)}{P(M_2 \mid D)}}_{\text{posterior odds}} = \underbrace{\frac{P(D \mid M_1)}{P(D \mid M_2)}}_{\text{Bayes factor}} \ \underbrace{\frac{P(M_1)}{P(M_2)}}_{\text{prior odds}}$$

## prediction

<span style = "color:white"> &nbsp; </span>

which future observations do we expect (after seeing some data)?

<span style = "color:white"> &nbsp; </span>

<div style = "float:left; width:45%;">

<span style = "color:firebrick">prior predictive</span>

$$ P(D) = \int P(\theta) \ P(D \mid \theta) \ \text{d}\theta $$

<span style = "color:white"> &nbsp; </span>

</div>
<div style = "float:right; width:45%;">

<span style = "color:firebrick">posterior predictive</span>

$$ P(D \mid D') = \int P(\theta \mid D') \ P(D \mid \theta) \ \text{d}\theta $$

<span style = "color:white"> &nbsp; </span>


</div>  


<span style = "color:white"> &nbsp; </span>
<span style = "color:white"> &nbsp; </span>


requires sampling distribution (more on this later)

special case: prior/posterior predictive $p$-value (model criticism)


## outlook

<span style = "color:white"> &nbsp; </span>

- focus on <span style = "color:firebrick">parameter estimation</span> first

- look at computational tools for efficiently calculating posterior $P(\theta \mid D)$

- use clever theory to reduce <span style = "color:firebrick">model comparison</span> to parameter estimation

# coin bias estimation

## likelihood function for several tosses

- success is 1; failure is 0
- pair $\tuple{k,n}$ is an outcome with $k$ success in $n$ flips

recap: <span style="color:firebrick">binomial distribution</span>:

$$ B(k ; n, \theta) = \binom{n}{k} \theta^{k} \, (1-\theta)^{n-k} $$


<span style = "color:firebrick">parameter estimation problem</span>

$$ P(\theta \mid k, n) = \frac{P(\theta) \ B(k ; n, \theta)}{\int P(\theta') \ B(k ; n, \theta') \ \text{d}\theta} $$ 


<div style = "position:absolute; top: 620px; right:60px;">
  hey!?! what about the $p$-problems, sampling distributions etc.?
</div>

## parameter estimation & normalized likelihoods

<span style = "color:firebrick">claim:</span> estimation of $P(\theta \mid D)$ is independent of assumptions about sample space and sample procedure

<span style = "color:firebrick">proof</span>

any normalizing constant $X$ cancels out:

$$
\begin{align*}
P(\theta \mid D) & = \frac{P(\theta) \ P(D \mid \theta)}{\int_{\theta'} P(\theta') \ P(D \mid \theta')} \\
& = \frac{ \frac{1}{X} \ P(\theta) \ P(D \mid \theta)}{ \ \frac{1}{X}\ \int_{\theta'} P(\theta') \ P(D \mid \theta')} \\
& = \frac{P(\theta) \ \frac{1}{X}\ P(D \mid \theta)}{  \int_{\theta'} P(\theta') \ \frac{1}{X}\ P(D \mid \theta')}
\end{align*}
$$

<div style="float: right; margin: 0px;"> <span style = "color:firebrick">$\Box$</span> </div>

## welcome infinity

what if $\theta$ is allowed to have any value $\theta \in [0;1]$?

<span style = "color:firebrick">two problems</span>

1. how to specify $P(\theta)$ in a concise way?
2. how to compute normalizing constant $\int_0^1  P(\theta) \ P(D \, | \, \theta) \, \text{d}\theta$?


<div class = "columns-2">

<span style = "color:firebrick">one solution</span>
  
  - use <span style = "color:firebrick">beta distribution</span> to specify prior $P(\theta)$ with some handy parameters
  - since this is the <span style = "color:firebrick">conjugate prior</span> to our likelihood function, computing posteriors is as easy as sleep

<span style = "color:white"> &nbsp; </span>

<span style = "color:white"> &nbsp; </span>
  
<img src="http://shrifeedesign.de/wp-content/uploads/2013/12/2fliegen.jpg" alt="KruschkeFig5.3" style="width: 250px;"/>
</div>

## beta distribution

2 shape parameters $a, b > 0$, defined over domain $[0;1]$

$$\text{Beta}(\theta \, | \, a, b) \propto \theta^{a-1} \, (1-\theta)^{b-1}$$

<div class = "centered">
<img src="pics/Kruschke_Fig6_1_BetaDistr.png" alt="KruschkeFig6.1" style="width: 380px;"/>
</div>

## conjugate distributions

<span style = "color:white"> &nbsp; </span>

if prior $P(\theta)$ and posterior $P(\theta \, | \, D)$ are of the same family, they <span style = "color:firebrick">conjugate</span>, and the prior $P(\theta)$ is called <span style = "color:firebrick">conjugate prior</span> for the likelihood function $P(D \, | \, \theta)$ from which the posterior $P(\theta \, | \, D)$ is derived

<span style = "color:firebrick">claim:</span> the beta distribution is the conjugate prior of a binomial likelihood function

<span style = "color:firebrick">proof</span>

$$ \begin{align*} 
P(\theta \mid \tuple{k, n}) & \propto B(k ; n, \theta) \ \text{Beta}(\theta \, | \, a, b) \\
P(\theta \mid \tuple{k, n}) & \propto \theta^{k} \, (1-\theta)^{n-k} \, \theta^{a-1} \, (1-\theta)^{b-1}  \ \  = \ \  \theta^{k + a - 1} \, (1-\theta)^{n-k +b -1} \\
P(\theta \mid \tuple{k, n}) & = \text{Beta}(\theta \, | \, k + a, n-k + b)
\end{align*}  $$

<div style="float: right; margin: 0px;"> <span style = "color:firebrick">$\Box$</span> </div>


## example applications

<div class = "centered">
<img src="pics/Kruschke_Fig6_4_BetaUpdates.png" alt="KruschkeFig6.4" style="width: 725px;"/>
</div>

# priors, likelihood & posterior

## Bayes' puppies

<span style = "color:white"> &nbsp; </span>

posterior is a "compromise" between prior and likelihood

$$\underbrace{P(\theta \, | \, D)}_{posterior} \propto \underbrace{P(\theta)}_{prior} \ \underbrace{P(D \, | \, \theta)}_{likelihood}$$

<span style = "color:white"> &nbsp; </span>



<div align = 'center'>
  <img src="http://4.bp.blogspot.com/-S47YpZTPIqk/T0ji_PrZ44I/AAAAAAAAAOA/buHdlQc2Si4/s1600/PuppiesBayesRule.jpg" alt="Bayes puppies" style="width: 650px;"/>
</div>


## influence of sample size on posterior

<div class = "centered">
<img src="pics/Kruschke_Fig5_2_influenceSampleSize.png" alt="KruschkeFig5.2" style="width: 610px;"/>
</div>

## influence of sample size on posterior

<div class = "centered">
<img src="pics/Kruschke_Fig5_3_influencePrior.png" alt="KruschkeFig5.3" style="width: 610px;"/>
</div>

# highest density intervals

## highest density interval

<span style = "color:white"> &nbsp; </span>

given distribution $P(\cdot) \in \Delta(X)$, the <span style = "color:black">95% highest density interval</span> is a subset $Y \subseteq X$ such that:

1. $P(Y) = .95$, and
2. no point outside of $Y$ is more likely than any point within.

<span style = "color:white"> dummy </span>

Intuition: range of values we are justified to belief in (categorically).

<div style = "position:absolute; top: 620px; right:60px;">
  caveat: NOT the same as the 2.5%-97.5% quantile range!!
</div>


## examples

<div class = "centered">
<img src="pics/Kruschke_Fig4_5_HDIExamples.png" alt="KruschkeFig5.3" style="width: 370px;"/>
</div>

## example

observed: $k = 7$ successes in $n = 24$ flips; 

prior: $\theta \sim \text{Beta}(1,1)$

```{r, echo = FALSE, results='hide', warning=FALSE, message=FALSE}

HDIofICDF = function( ICDFname , credMass=0.95 , tol=1e-8 , ... ) {
  # Arguments:
  #   ICDFname is R's name for the inverse cumulative density function
  #     of the distribution.
  #   credMass is the desired mass of the HDI region.
  #   tol is passed to R's optimize function.
  # Return value:
  #   Highest density iterval (HDI) limits in a vector.
  # Example of use: For determining HDI of a beta(30,12) distribution, type
  #   HDIofICDF( qbeta , shape1 = 30 , shape2 = 12 )
  #   Notice that the parameters of the ICDFname must be explicitly named;
  #   e.g., HDIofICDF( qbeta , 30 , 12 ) does not work.
  # Adapted and corrected from Greg Snow's TeachingDemos package.
  incredMass =  1.0 - credMass
  intervalWidth = function( lowTailPr , ICDFname , credMass , ... ) {
    ICDFname( credMass + lowTailPr , ... ) - ICDFname( lowTailPr , ... )
  }
  optInfo = optimize( intervalWidth , c( 0 , incredMass ) , ICDFname=ICDFname ,
                      credMass=credMass , tol=tol , ... )
  HDIlowTailPr = optInfo$minimum
  return( c( ICDFname( HDIlowTailPr , ... ) ,
             ICDFname( credMass + HDIlowTailPr , ... ) ) )
}

plotData = data.frame(theta = seq(0.01,1, by = 0.01),
                      post = dbeta(seq(0.01,1, by = 0.01), 8, 18 ))
hdi = HDIofICDF( qbeta , shape1 = 8 , shape2 = 18 )
hdiData = data.frame(theta = rep(hdi, each = 2),
                     post = c(0,dbeta(hdi, 8, 18), 0))
ggplot(plotData, aes(x = theta, y = post)) + xlim(0,1) + geom_line(color = "black") + ylab("posterior") +
  geom_line(data = hdiData, aes(x = theta, y = post), color = "skyblue", size = 1) +
  geom_text(x = mean(hdi), y = 1, label = "HDI: 0.14 - 0.48")

```


# the road ahead

## BDA more generally

problems:

- conjugate priors are not always available:
    - likelihood functions can come from unbending beasts:
        - complex hierarchical models (e.g., regression)
        - custom-made stuff (e.g., probabilistic grammars)
- even when available, they may not be what we want:
    - prior beliefs could be different from what a conjugate prior can capture

<span style = "color:white"> dummy </span>
    
solution:

- approximate posterior distribution by smart numerical simulations


# fini

## outlook

<span style = "color:white"> &nbsp; </span>

<span style = "color:firebrick">Tuesday</span>

- introduction to MCMC methods

<span style = "color:white"> &nbsp; </span>

<span style = "color:firebrick">Friday</span>

- introduction to JAGS


## to prevent boredom

<span style = "color:white"> &nbsp; </span>

<span style = "color:firebrick">obligatory</span>

- prepare Kruschke chapter 7 

- finish first homework set: due Friday before class